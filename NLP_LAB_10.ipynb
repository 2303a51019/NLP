{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rcUb66R9mjhUEP3WcWMavF4jhIGf5i4M",
      "authorship_tag": "ABX9TyPYt5JJBnAc3BzHrERG3rcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51019/NLP/blob/main/NLP_LAB_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Topic Modeling with NMF"
      ],
      "metadata": {
        "id": "99b4Ba8lS-c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, download\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS_BY_6AOw2J",
        "outputId": "75abbcbe-04fc-4a1f-ebc0-b765926063c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Load Dataset (BBC News or any text dataset)\n",
        "df = pd.read_csv(\"/content/bbc_news.csv\")  # ensure this file exists in the working directory\n",
        "texts = df['description'].astype(str).tolist()"
      ],
      "metadata": {
        "id": "YTWwH5hAO4r_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Preprocess Text\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    words = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "processed_texts = [preprocess(t) for t in texts]\n"
      ],
      "metadata": {
        "id": "6Um1rXy3SSc6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4️⃣ Apply NMF for 5 Topics\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.9)\n",
        "tfidf = tfidf_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "nmf_model = NMF(n_components=5, random_state=42)\n",
        "nmf_topics = nmf_model.fit_transform(tfidf)\n",
        "\n",
        "# Display top 10 words per topic\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(nmf_model.components_):\n",
        "    print(f\"\\nNMF Topic {topic_idx + 1}:\")\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU1su5trSr-r",
        "outputId": "7e642952-7f06-45f8-8bfd-ea379d807a63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NMF Topic 1:\n",
            "say police family man official former president want attack could\n",
            "\n",
            "NMF Topic 2:\n",
            "world england cup woman win final watch first wale championship\n",
            "\n",
            "NMF Topic 3:\n",
            "year bbc people uk new two first one government ukraine\n",
            "\n",
            "NMF Topic 4:\n",
            "day seven past going attention closely paying image selection taken\n",
            "\n",
            "NMF Topic 5:\n",
            "league manchester city premier united champion win liverpool arsenal season\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Compare with LDA\n",
        "count_vectorizer = CountVectorizer(max_features=2000, min_df=5, max_df=0.9)\n",
        "count = count_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_topics = lda_model.fit_transform(count)\n",
        "\n",
        "feature_names_lda = count_vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"\\nLDA Topic {topic_idx + 1}:\")\n",
        "    print(\" \".join([feature_names_lda[i] for i in topic.argsort()[:-11:-1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQLCBg70Sznf",
        "outputId": "d178e831-76f6-49de-9107-63d209a8de02"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA Topic 1:\n",
            "say uk people could year england cost pay new service\n",
            "\n",
            "LDA Topic 2:\n",
            "bbc say ukraine russia russian tell war president israel city\n",
            "\n",
            "LDA Topic 3:\n",
            "world england cup win league first final day manchester woman\n",
            "\n",
            "LDA Topic 4:\n",
            "say minister government election uk party leader former first new\n",
            "\n",
            "LDA Topic 5:\n",
            "say police people year two died show one woman found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: WordNet Similarity"
      ],
      "metadata": {
        "id": "23F5tPhfTCGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Import WordNet Tools\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# 2️⃣ Choose two words from same topic\n",
        "word1 = \"economy\"\n",
        "word2 = \"finance\"\n",
        "\n",
        "# 3️⃣ Get Synsets (Word Senses)\n",
        "syn1 = wn.synsets(word1)[0]\n",
        "syn2 = wn.synsets(word2)[0]\n",
        "\n",
        "# 4️⃣ Compute Path and Wu-Palmer Similarity\n",
        "path_sim = syn1.path_similarity(syn2)\n",
        "wup_sim = syn1.wup_similarity(syn2)\n",
        "\n",
        "# 5️⃣ Display Results\n",
        "print(f\"\\nPath Similarity between '{word1}' and '{word2}': {path_sim}\")\n",
        "print(f\"Wu-Palmer Similarity between '{word1}' and '{word2}': {wup_sim}\")\n",
        "\n",
        "if wup_sim > 0.6:\n",
        "    print(\"These words are semantically close (similar meaning).\")\n",
        "else:\n",
        "    print(\"These words are not closely related semantically.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nn0NX4ATQ-L",
        "outputId": "33b7fdc3-bf57-4efc-82ae-f60ca73101cd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Path Similarity between 'economy' and 'finance': 0.09090909090909091\n",
            "Wu-Palmer Similarity between 'economy' and 'finance': 0.2857142857142857\n",
            "These words are not closely related semantically.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Pairwise Document Similarity"
      ],
      "metadata": {
        "id": "6vD2hQyrTHuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Select Three Documents\n",
        "doc1 = processed_texts[0]\n",
        "doc2 = processed_texts[1]\n",
        "doc3 = processed_texts[2]\n",
        "\n",
        "# 2️⃣ Convert Documents to Sets of Words\n",
        "set1, set2, set3 = set(doc1.split()), set(doc2.split()), set(doc3.split())\n",
        "\n",
        "# 3️⃣ Define Jaccard Similarity Function\n",
        "def jaccard_similarity(a, b):\n",
        "    intersection = len(a.intersection(b))\n",
        "    union = len(a.union(b))\n",
        "    return intersection / union\n",
        "\n",
        "# 4️⃣ Compute Pairwise Similarities\n",
        "sim_12 = jaccard_similarity(set1, set2)\n",
        "sim_13 = jaccard_similarity(set1, set3)\n",
        "sim_23 = jaccard_similarity(set2, set3)\n",
        "\n",
        "# 5️⃣ Display Results\n",
        "print(\"\\nJaccard Similarities:\")\n",
        "print(f\"Doc1 & Doc2: {sim_12:.3f}\")\n",
        "print(f\"Doc1 & Doc3: {sim_13:.3f}\")\n",
        "print(f\"Doc2 & Doc3: {sim_23:.3f}\")\n",
        "\n",
        "# Identify which pair is most and least similar\n",
        "sims = {'Doc1-Doc2': sim_12, 'Doc1-Doc3': sim_13, 'Doc2-Doc3': sim_23}\n",
        "most_similar = max(sims, key=sims.get)\n",
        "least_similar = min(sims, key=sims.get)\n",
        "\n",
        "print(f\"\\nMost similar pair: {most_similar}\")\n",
        "print(f\"Least similar pair: {least_similar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdsOwg3STVwy",
        "outputId": "532bbbd2-da9a-4922-a4db-2a7d30c583b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jaccard Similarities:\n",
            "Doc1 & Doc2: 0.000\n",
            "Doc1 & Doc3: 0.053\n",
            "Doc2 & Doc3: 0.000\n",
            "\n",
            "Most similar pair: Doc1-Doc3\n",
            "Least similar pair: Doc1-Doc2\n"
          ]
        }
      ]
    }
  ]
}